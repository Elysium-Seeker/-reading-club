"""
ğŸ“š é˜…è¯»è®¡åˆ’åä½œç®¡ç†å·¥å…· - Python æœåŠ¡å™¨
ä½¿ç”¨ Python æ ‡å‡†åº“ï¼Œæ— éœ€å®‰è£…ä»»ä½•é¢å¤–ä¾èµ–ã€‚

å¯åŠ¨æ–¹å¼: python server.py
è®¿é—®åœ°å€: http://localhost:3000
"""

import http.server
import json
import os
import uuid
from datetime import datetime, timezone
from urllib.parse import urlparse, parse_qs
import socketserver
import urllib.request
import urllib.error
import urllib.parse
import concurrent.futures
import re
import html as html_lib

SEARCH_USER_AGENT = 'ReadingClubApp/1.0 (+https://openlibrary.org)'
DOUBAN_CACHE = {}

PORT = int(os.environ.get('PORT', 3000))
DATA_FILE = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'data', 'books.json')
PUBLIC_DIR = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'public')


def normalize_text(value):
    text = str(value or '').strip().lower()
    text = re.sub(r'\s+', ' ', text)
    return text


def normalize_key(title, author):
    raw = f"{normalize_text(title)}|{normalize_text(author)}"
    return re.sub(r'[^\w\u4e00-\u9fff]+', '', raw)


def to_float(value):
    try:
        if value is None:
            return None
        return float(value)
    except (TypeError, ValueError):
        return None


def score_match(query_title, query_author, candidate_title, candidate_author):
    title_q = normalize_text(query_title)
    author_q = normalize_text(query_author)
    title_c = normalize_text(candidate_title)
    author_c = normalize_text(candidate_author)

    score = 0
    if title_q:
        if title_c == title_q:
            score += 85
        elif title_c.startswith(title_q):
            score += 55
        elif title_q in title_c:
            score += 35

    if author_q:
        if author_c == author_q:
            score += 35
        elif author_c.startswith(author_q):
            score += 22
        elif author_q in author_c:
            score += 15

    return score


def merge_resources(resources):
    def normalize_url(url):
        fixed = str(url or '').strip()
        if fixed.startswith('http://books.google.com'):
            fixed = fixed.replace('http://', 'https://', 1)
        if fixed.startswith('http://play.google.com'):
            fixed = fixed.replace('http://', 'https://', 1)
        if fixed.startswith('http://archive.org'):
            fixed = fixed.replace('http://', 'https://', 1)
        return fixed

    merged = []
    seen = set()
    for item in resources or []:
        url = normalize_url(item.get('url', ''))
        if not url or url in seen:
            continue
        seen.add(url)
        merged.append({
            'name': item.get('name', 'èµ„æºé“¾æ¥'),
            'url': url,
            'type': item.get('type', 'è¯¦æƒ…')
        })
    return merged[:8]


def append_discovery_resources(resources, title, author):
    query = urllib.parse.quote(f"{title} {author}".strip())
    out = list(resources or [])

    # ç»Ÿä¸€åˆ¤å®šæ˜¯å¦å·²ç»æœ‰å¯ç›´æ¥é˜…è¯»/å€Ÿé˜…çš„èµ„æº
    has_readable = any((r.get('type') in ('ç”µå­ä¹¦', 'åœ¨çº¿é˜…è¯»', 'å€Ÿé˜…')) for r in out)
    if has_readable:
        return merge_resources(out)

    # å…œåº•ï¼šåˆæ³•å¹³å°æ£€ç´¢å…¥å£ï¼Œé¿å…â€œå®Œå…¨æ‰¾ä¸åˆ°â€
    out.extend([
        {
            'name': 'è±†ç“£è¯»ä¹¦æ£€ç´¢',
            'url': f'https://m.douban.com/search/?query={query}&type=book',
            'type': 'æ£€ç´¢'
        },
        {
            'name': 'å¾®ä¿¡è¯»ä¹¦æ£€ç´¢',
            'url': f'https://weread.qq.com/web/search/books?keyword={query}',
            'type': 'æ£€ç´¢'
        },
        {
            'name': 'Google Play Books æ£€ç´¢',
            'url': f'https://play.google.com/store/search?q={query}&c=books',
            'type': 'æ£€ç´¢'
        }
    ])

    return merge_resources(out)


def has_real_synopsis(text):
    content = str(text or '').strip()
    if not content:
        return False
    return not content.startswith('æš‚æ— å¯å…¬å¼€æŠ“å–çš„è¯¦ç»†ç®€ä»‹')


def contains_cjk(text):
    value = str(text or '')
    return bool(re.search(r'[\u4e00-\u9fff]', value))


def clean_html_text(raw_html):
    text = re.sub(r'<br\s*/?>', '\n', raw_html, flags=re.I)
    text = re.sub(r'</p\s*>', '\n', text, flags=re.I)
    text = re.sub(r'<[^>]+>', '', text)
    text = html_lib.unescape(text)
    text = re.sub(r'[ \t\r\f\v]+', ' ', text)
    text = re.sub(r'\n+', '\n', text).strip()
    return text


def fetch_douban_best_metadata(title, author=''):
    cache_key = normalize_key(title, author)
    if cache_key in DOUBAN_CACHE:
        return DOUBAN_CACHE[cache_key]

    try:
        query = urllib.parse.quote(f"{title} {author}".strip())
        search_url = f"https://m.douban.com/search/?query={query}&type=book"
        req = urllib.request.Request(search_url, headers={'User-Agent': 'Mozilla/5.0'})
        search_html = urllib.request.urlopen(req, timeout=8).read().decode('utf-8', 'ignore')

        subject_ids = re.findall(r'href="/book/subject/(\d+)/"', search_html)
        unique_ids = []
        for sid in subject_ids:
            if sid not in unique_ids:
                unique_ids.append(sid)
        unique_ids = unique_ids[:5]

        best = None
        best_score = -1

        for sid in unique_ids:
            detail_url = f"https://book.douban.com/subject/{sid}/"
            dreq = urllib.request.Request(detail_url, headers={'User-Agent': 'Mozilla/5.0'})
            html = urllib.request.urlopen(dreq, timeout=8).read().decode('utf-8', 'ignore')

            title_match = re.search(r'<span\s+property="v:itemreviewed">([^<]+)</span>', html)
            db_title = html_lib.unescape(title_match.group(1).strip()) if title_match else ''

            info_match = re.search(r'<div\s+id="info"[^>]*>([\s\S]*?)</div>', html)
            info_text = clean_html_text(info_match.group(1)) if info_match else ''
            author_match = re.search(r'ä½œè€…[:ï¼š]\s*([^\n/]+)', info_text)
            db_author = author_match.group(1).strip() if author_match else ''

            score = score_match(title, author, db_title, db_author)

            rating_match = re.search(r'<strong\s+class="ll rating_num\s*"[^>]*>\s*([0-9.]+)\s*</strong>', html)
            db_rating = to_float(rating_match.group(1)) if rating_match else None
            if db_rating:
                score += 6

            intros = re.findall(r'<div\s+class="intro">([\s\S]*?)</div>', html)
            intro_texts = [clean_html_text(x) for x in intros if clean_html_text(x)]
            intro = max(intro_texts, key=len) if intro_texts else ''
            if intro:
                score += 8

            candidate = {
                'title': db_title,
                'author': db_author,
                'synopsis': intro[:420],
                'rating': round(db_rating, 1) if db_rating else None,
                'ratingSource': 'è±†ç“£' if db_rating else '',
                'source': 'è±†ç“£',
                'resource': {
                    'name': 'è±†ç“£é¡µé¢',
                    'url': detail_url,
                    'type': 'è¯¦æƒ…'
                }
            }

            if score > best_score:
                best_score = score
                best = candidate

        # ä½åŒ¹é…ç»“æœç›´æ¥å¿½ç•¥ï¼Œé¿å…è¯¯å¡«
        if best_score < 30:
            DOUBAN_CACHE[cache_key] = None
            return None

        DOUBAN_CACHE[cache_key] = best
        return best
    except Exception:
        DOUBAN_CACHE[cache_key] = None
        return None


def build_openlibrary_resources(doc):
    resources = []
    title = doc.get('title', '')
    author = ', '.join(doc.get('author_name', [])[:1]) if doc.get('author_name') else ''
    query = urllib.parse.quote(f"{title} {author}".strip())

    if doc.get('key'):
        resources.append({
            'name': 'Open Library é¡µé¢',
            'url': f"https://openlibrary.org{doc['key']}",
            'type': 'è¯¦æƒ…'
        })

    ia_ids = doc.get('ia', []) or []
    if ia_ids:
        resources.append({
            'name': 'Internet Archive å€Ÿé˜…/é¢„è§ˆ',
            'url': f"https://archive.org/details/{ia_ids[0]}",
            'type': 'å€Ÿé˜…'
        })

    if doc.get('ebook_access') in ('public', 'borrowable', 'printdisabled') and doc.get('key'):
        resources.append({
            'name': 'Open Library ç”µå­ç‰ˆå…¥å£',
            'url': f"https://openlibrary.org{doc.get('key', '')}",
            'type': 'ç”µå­ä¹¦'
        })

    resources.append({
        'name': 'Google Books æ£€ç´¢',
        'url': f'https://books.google.com/books?q={query}',
        'type': 'æ£€ç´¢'
    })
    return merge_resources(resources)


def build_google_resources(item):
    volume = item.get('volumeInfo', {})
    access = item.get('accessInfo', {})
    resources = []

    if volume.get('infoLink'):
        resources.append({'name': 'Google Books é¡µé¢', 'url': volume['infoLink'], 'type': 'è¯¦æƒ…'})
    if volume.get('previewLink'):
        resources.append({'name': 'Google Books é¢„è§ˆ', 'url': volume['previewLink'], 'type': 'é¢„è§ˆ'})
    if access.get('webReaderLink'):
        resources.append({'name': 'Google Web Reader', 'url': access['webReaderLink'], 'type': 'åœ¨çº¿é˜…è¯»'})

    epub = (access.get('epub') or {})
    pdf = (access.get('pdf') or {})
    if epub.get('isAvailable') and epub.get('acsTokenLink'):
        resources.append({'name': 'Google EPUB è·å–', 'url': epub['acsTokenLink'], 'type': 'ç”µå­ä¹¦'})
    if pdf.get('isAvailable') and pdf.get('acsTokenLink'):
        resources.append({'name': 'Google PDF è·å–', 'url': pdf['acsTokenLink'], 'type': 'ç”µå­ä¹¦'})

    return merge_resources(resources)


def build_gutendex_resources(book):
    formats = book.get('formats', {}) or {}
    resources = []

    for fmt_key, fmt_url in formats.items():
        if not fmt_url or fmt_url.endswith('.zip'):
            continue
        if 'text/html' in fmt_key:
            resources.append({'name': 'Project Gutenberg åœ¨çº¿é˜…è¯»', 'url': fmt_url, 'type': 'åœ¨çº¿é˜…è¯»'})
        elif 'application/epub+zip' in fmt_key:
            resources.append({'name': 'Project Gutenberg EPUB', 'url': fmt_url, 'type': 'ç”µå­ä¹¦'})
        elif 'application/pdf' in fmt_key:
            resources.append({'name': 'Project Gutenberg PDF', 'url': fmt_url, 'type': 'ç”µå­ä¹¦'})

    if book.get('id'):
        resources.append({
            'name': 'Gutendex è¯¦æƒ…',
            'url': f"https://gutendex.com/books/{book['id']}",
            'type': 'è¯¦æƒ…'
        })

    return merge_resources(resources)


def fetch_openlibrary_candidates(title, author=''):
    fields = ','.join([
        'key', 'title', 'author_name', 'first_publish_year', 'cover_i',
        'ratings_average', 'ratings_count', 'subject', 'ia', 'ebook_access'
    ])
    url = f"https://openlibrary.org/search.json?title={urllib.parse.quote(title)}&limit=12&fields={fields}"
    if author:
        url += f"&author={urllib.parse.quote(author)}"

    data = fetch_json(url, timeout=7)
    results = []
    for doc in data.get('docs', []):
        book_title = doc.get('title', '')
        book_author = ', '.join(doc.get('author_name', [])[:2]) if doc.get('author_name') else ''

        rating = to_float(doc.get('ratings_average'))
        rating_count = int(doc.get('ratings_count', 0) or 0)
        score = score_match(title, author, book_title, book_author)
        if rating:
            score += 8
        if rating_count:
            score += min(12, rating_count // 40)
        if doc.get('cover_i'):
            score += 5
        if doc.get('first_publish_year'):
            score += 2

        cover = ''
        if doc.get('cover_i'):
            cover = f"https://covers.openlibrary.org/b/id/{doc['cover_i']}-M.jpg"

        results.append({
            'title': book_title,
            'author': book_author,
            'synopsis': '',
            'rating': round(rating, 1) if rating else None,
            'ratingSource': 'Open Library' if rating else '',
            'category': map_category(doc.get('subject', [])[:5]),
            'cover': cover,
            'year': doc.get('first_publish_year'),
            'source': 'Open Library',
            'resources': build_openlibrary_resources(doc),
            '_score': score,
            '_work_key': doc.get('key', '')
        })
    return results


def fetch_googlebooks_candidates(title, author=''):
    query_parts = [f"intitle:{title}"]
    if author:
        query_parts.append(f"inauthor:{author}")
    query = urllib.parse.quote(' '.join(query_parts))
    url = f"https://www.googleapis.com/books/v1/volumes?q={query}&maxResults=12&printType=books"

    data = fetch_json(url, timeout=7)
    items = data.get('items', [])
    results = []

    for item in items:
        volume = item.get('volumeInfo', {})
        book_title = volume.get('title', '')
        book_author = ', '.join(volume.get('authors', [])[:2]) if volume.get('authors') else ''
        rating = to_float(volume.get('averageRating'))
        ratings_count = int(volume.get('ratingsCount', 0) or 0)

        score = score_match(title, author, book_title, book_author)
        if rating:
            score += 7
        if ratings_count:
            score += min(10, ratings_count // 50)
        if volume.get('description'):
            score += 4
        if volume.get('imageLinks', {}).get('thumbnail'):
            score += 3

        image_links = volume.get('imageLinks', {}) or {}
        cover = image_links.get('thumbnail', '') or image_links.get('smallThumbnail', '')
        if cover.startswith('http://'):
            cover = cover.replace('http://', 'https://', 1)

        year = None
        published_date = str(volume.get('publishedDate', ''))
        if published_date[:4].isdigit():
            year = int(published_date[:4])

        categories = volume.get('categories', [])[:3]
        description = str(volume.get('description', '')).strip()[:260]

        results.append({
            'title': book_title,
            'author': book_author,
            'synopsis': description,
            'rating': round(rating, 1) if rating else None,
            'ratingSource': 'Google Books' if rating else '',
            'category': map_category(categories),
            'cover': cover,
            'year': year,
            'source': 'Google Books',
            'resources': build_google_resources(item),
            '_score': score,
            '_work_key': ''
        })

    return results


def fetch_gutendex_candidates(title, author=''):
    query = urllib.parse.quote(f"{title} {author}".strip())
    url = f"https://gutendex.com/books?search={query}"

    data = fetch_json(url, timeout=7)
    results = []
    for book in (data.get('results', []) or [])[:12]:
        book_title = book.get('title', '')
        book_author = ', '.join([a.get('name', '') for a in (book.get('authors') or []) if a.get('name')][:2])
        score = score_match(title, author, book_title, book_author)
        if book.get('download_count'):
            score += min(8, int(book.get('download_count', 0)) // 200)

        subjects = [s for s in (book.get('subjects') or [])[:4]]
        synopsis = ''
        if subjects:
            synopsis = f"ä¸»é¢˜: {' / '.join(subjects[:3])}"

        results.append({
            'title': book_title,
            'author': book_author,
            'synopsis': synopsis,
            'rating': None,
            'ratingSource': '',
            'category': map_category(subjects),
            'cover': '',
            'year': None,
            'source': 'Gutendex',
            'resources': build_gutendex_resources(book),
            '_score': score,
            '_work_key': ''
        })
    return results


def fetch_openlibrary_best_doc(title, author=''):
    fields = ','.join([
        'key', 'title', 'author_name', 'first_publish_year', 'cover_i',
        'ratings_average', 'ratings_count', 'subject', 'ia', 'ebook_access'
    ])
    url = f"https://openlibrary.org/search.json?title={urllib.parse.quote(title)}&limit=10&fields={fields}"
    if author:
        url += f"&author={urllib.parse.quote(author)}"

    data = fetch_json(url, timeout=6)
    best_doc = None
    best_score = -1
    for doc in data.get('docs', [])[:10]:
        cand_title = doc.get('title', '')
        cand_author = ', '.join(doc.get('author_name', [])[:2]) if doc.get('author_name') else ''
        score = score_match(title, author, cand_title, cand_author)
        if doc.get('ratings_average'):
            score += 4
        if doc.get('cover_i'):
            score += 2
        if score > best_score:
            best_score = score
            best_doc = doc
    return best_doc


def fetch_googlebooks_best_item(title, author=''):
    query_parts = [f"intitle:{title}"]
    if author:
        query_parts.append(f"inauthor:{author}")
    query = urllib.parse.quote(' '.join(query_parts))
    url = f"https://www.googleapis.com/books/v1/volumes?q={query}&maxResults=10&printType=books"
    data = fetch_json(url, timeout=6)

    best_item = None
    best_score = -1
    for item in (data.get('items', []) or [])[:10]:
        volume = item.get('volumeInfo', {})
        cand_title = volume.get('title', '')
        cand_author = ', '.join(volume.get('authors', [])[:2]) if volume.get('authors') else ''
        score = score_match(title, author, cand_title, cand_author)
        if volume.get('description'):
            score += 5
        if volume.get('imageLinks', {}).get('thumbnail'):
            score += 2
        if volume.get('averageRating'):
            score += 2
        if score > best_score:
            best_score = score
            best_item = item
    return best_item


def enrich_candidate_metadata(item, query_title='', query_author=''):
    enriched = dict(item)

    # å…ˆå°è¯• Google Books è¯¦æƒ…è¡¥å……
    try:
        gb_item = fetch_googlebooks_best_item(
            enriched.get('title') or query_title,
            enriched.get('author') or query_author
        )
        if gb_item:
            volume = gb_item.get('volumeInfo', {})

            if not enriched.get('synopsis'):
                enriched['synopsis'] = str(volume.get('description', '')).strip()[:320]

            if (not enriched.get('category')) or enriched.get('category') == 'æ–‡å­¦å°è¯´':
                categories = volume.get('categories', [])[:4]
                mapped = map_category(categories)
                if mapped:
                    enriched['category'] = mapped

            if not enriched.get('cover'):
                image_links = volume.get('imageLinks', {}) or {}
                cover = image_links.get('thumbnail', '') or image_links.get('smallThumbnail', '')
                if cover.startswith('http://'):
                    cover = cover.replace('http://', 'https://', 1)
                enriched['cover'] = cover

            if (not enriched.get('rating')) and volume.get('averageRating') is not None:
                rating = to_float(volume.get('averageRating'))
                if rating:
                    enriched['rating'] = round(rating, 1)
                    enriched['ratingSource'] = 'Google Books'

            enriched['resources'] = merge_resources((enriched.get('resources') or []) + build_google_resources(gb_item))
    except Exception:
        pass

    # å†å°è¯• Open Library work è¯¦æƒ…è¡¥å……
    try:
        ol_doc = fetch_openlibrary_best_doc(
            enriched.get('title') or query_title,
            enriched.get('author') or query_author
        )
        if ol_doc:
            if not enriched.get('synopsis') and ol_doc.get('key'):
                enriched['synopsis'] = fetch_work_description(ol_doc.get('key'))

            if (not enriched.get('category')) or enriched.get('category') == 'æ–‡å­¦å°è¯´':
                enriched['category'] = map_category(ol_doc.get('subject', [])[:6])

            if not enriched.get('cover') and ol_doc.get('cover_i'):
                enriched['cover'] = f"https://covers.openlibrary.org/b/id/{ol_doc['cover_i']}-M.jpg"

            if (not enriched.get('rating')) and ol_doc.get('ratings_average') is not None:
                rating = to_float(ol_doc.get('ratings_average'))
                if rating:
                    enriched['rating'] = round(rating, 1)
                    enriched['ratingSource'] = 'Open Library'

            enriched['resources'] = merge_resources((enriched.get('resources') or []) + build_openlibrary_resources(ol_doc))
    except Exception:
        pass

    # ä¸­æ–‡ä¹¦è¡¥å……ï¼šè±†ç“£ç®€ä»‹ä¸è¯„åˆ†ï¼ˆæœ€ä½³åŒ¹é…ï¼‰
    try:
        if contains_cjk(enriched.get('title') or query_title):
            if (not enriched.get('rating')) or (not has_real_synopsis(enriched.get('synopsis', ''))):
                douban = fetch_douban_best_metadata(
                    enriched.get('title') or query_title,
                    enriched.get('author') or query_author
                )
                if douban:
                    if (not has_real_synopsis(enriched.get('synopsis', ''))) and has_real_synopsis(douban.get('synopsis', '')):
                        enriched['synopsis'] = douban.get('synopsis', '')
                    if (not enriched.get('rating')) and douban.get('rating'):
                        enriched['rating'] = douban.get('rating')
                        enriched['ratingSource'] = douban.get('ratingSource', 'è±†ç“£')
                    enriched['resources'] = merge_resources((enriched.get('resources') or []) + [douban.get('resource', {})])
                    if enriched.get('source'):
                        if 'è±†ç“£' not in enriched['source']:
                            enriched['source'] = f"{enriched['source']} / è±†ç“£"
                    else:
                        enriched['source'] = 'è±†ç“£'
    except Exception:
        pass

    # æœ€åå…œåº•ï¼šä¿è¯å‰ç«¯èƒ½æ‹¿åˆ°å¯å±•ç¤ºçš„ç®€ä»‹æ–‡æœ¬
    if not (enriched.get('synopsis') or '').strip():
        parts = []
        if enriched.get('author'):
            parts.append(f"ä½œè€…ï¼š{enriched.get('author')}")
        if enriched.get('year'):
            parts.append(f"å‡ºç‰ˆå¹´ä»½ï¼š{enriched.get('year')}")
        if enriched.get('category'):
            parts.append(f"åˆ†ç±»ï¼š{enriched.get('category')}")
        if enriched.get('source'):
            parts.append(f"æ•°æ®æ¥æºï¼š{enriched.get('source')}")

        suffix = 'ï¼›'.join(parts)
        if suffix:
            enriched['synopsis'] = f"æš‚æ— å¯å…¬å¼€æŠ“å–çš„è¯¦ç»†ç®€ä»‹ã€‚{suffix}ã€‚å¯ç‚¹å‡»ä¸‹æ–¹èµ„æºé“¾æ¥æŸ¥çœ‹è¯¦æƒ…é¡µæˆ–åœ¨çº¿é¢„è§ˆã€‚"
        else:
            enriched['synopsis'] = 'æš‚æ— å¯å…¬å¼€æŠ“å–çš„è¯¦ç»†ç®€ä»‹ï¼Œå¯ç‚¹å‡»ä¸‹æ–¹èµ„æºé“¾æ¥æŸ¥çœ‹è¯¦æƒ…é¡µæˆ–åœ¨çº¿é¢„è§ˆã€‚'

    return enriched


def merge_candidates(candidates):
    merged = {}
    for item in candidates:
        key = normalize_key(item.get('title', ''), item.get('author', ''))
        if not key:
            continue

        if key not in merged:
            merged[key] = item
            merged[key]['sources'] = [item.get('source', '')] if item.get('source') else []
            continue

        current = merged[key]
        if item.get('_score', 0) > current.get('_score', 0):
            preferred = item
            backup = current
        else:
            preferred = current
            backup = item

        combined = dict(preferred)
        combined['synopsis'] = preferred.get('synopsis') or backup.get('synopsis', '')
        if len(backup.get('synopsis', '')) > len(combined.get('synopsis', '')):
            combined['synopsis'] = backup.get('synopsis', '')

        if not combined.get('cover'):
            combined['cover'] = backup.get('cover', '')
        if not combined.get('rating') and backup.get('rating'):
            combined['rating'] = backup.get('rating')
            combined['ratingSource'] = backup.get('ratingSource', '')
        if not combined.get('year') and backup.get('year'):
            combined['year'] = backup.get('year')
        if not combined.get('category') or combined.get('category') == 'æ–‡å­¦å°è¯´':
            if backup.get('category'):
                combined['category'] = backup.get('category')

        combined['resources'] = merge_resources((current.get('resources') or []) + (item.get('resources') or []))

        src_set = set((current.get('sources') or []) + (item.get('sources') or []) + ([current.get('source')] if current.get('source') else []) + ([item.get('source')] if item.get('source') else []))
        combined['sources'] = [s for s in src_set if s]
        combined['source'] = ' / '.join(combined['sources'])

        merged[key] = combined

    return list(merged.values())


def search_book_info(title, author=""):
    """èšåˆå¤šä¸ªå…¬å¼€ API æœç´¢ä¹¦ç±å¹¶åˆå¹¶ç»“æœã€‚"""
    try:
        with concurrent.futures.ThreadPoolExecutor(max_workers=3) as executor:
            futures = [
                executor.submit(fetch_openlibrary_candidates, title, author),
                executor.submit(fetch_googlebooks_candidates, title, author),
                executor.submit(fetch_gutendex_candidates, title, author),
            ]

            all_candidates = []
            for future in concurrent.futures.as_completed(futures):
                try:
                    all_candidates.extend(future.result() or [])
                except Exception:
                    continue

        if not all_candidates:
            return []

        merged = merge_candidates(all_candidates)
        for index, item in enumerate(merged[:8]):
            need_enrich = (
                not item.get('synopsis')
                or not item.get('cover')
                or (not item.get('rating'))
                or item.get('category') in ('', 'æ–‡å­¦å°è¯´')
            )
            if need_enrich and index < 6:
                merged[index] = enrich_candidate_metadata(item, title, author)
            elif (not item.get('synopsis')) and item.get('_work_key'):
                merged[index]['synopsis'] = fetch_work_description(item.get('_work_key'))

        merged.sort(
            key=lambda i: (
                1 if has_real_synopsis(i.get('synopsis', '')) else 0,
                i.get('_score', 0),
                i.get('rating') or 0
            ),
            reverse=True
        )
        results = []
        for item in merged[:8]:
            final_resources = append_discovery_resources(
                item.get('resources', []),
                item.get('title', ''),
                item.get('author', '')
            )
            results.append({
                'title': item.get('title', ''),
                'author': item.get('author', ''),
                'synopsis': item.get('synopsis', ''),
                'rating': item.get('rating'),
                'ratingSource': item.get('ratingSource', ''),
                'category': item.get('category', 'æ–‡å­¦å°è¯´'),
                'cover': item.get('cover', ''),
                'year': item.get('year'),
                'source': item.get('source', ''),
                'resources': final_resources
            })
        return results
    except Exception as e:
        return {'error': str(e)}


def fetch_json(url, timeout=6):
    req = urllib.request.Request(url, headers={'User-Agent': SEARCH_USER_AGENT})
    with urllib.request.urlopen(req, timeout=timeout) as response:
        return json.loads(response.read().decode('utf-8'))


def fetch_work_description(work_key):
    if not work_key:
        return ''
    try:
        work_url = f"https://openlibrary.org{work_key}.json"
        work_data = fetch_json(work_url, timeout=5)
        desc = work_data.get('description', '')
        if isinstance(desc, dict):
            desc = desc.get('value', '')
        text = str(desc).strip()
        return text[:260] if text else ''
    except Exception:
        return ''


def map_category(subjects):
    if not subjects:
        return 'æ–‡å­¦å°è¯´'
    joined = ' '.join(subjects).lower()
    mapping = [
        ('science fiction fantasy dystopia', 'ç§‘å¹»å¥‡å¹»'),
        ('mystery detective crime thriller', 'æ¨ç†æ‚¬ç–‘'),
        ('history biography memoir', 'å†å²ä¼ è®°'),
        ('philosophy ethics', 'å“²å­¦æ€æƒ³'),
        ('sociology politics culture society', 'ç¤¾ä¼šç§‘å­¦'),
        ('science physics biology chemistry', 'è‡ªç„¶ç§‘å­¦'),
        ('psychology mental', 'å¿ƒç†å­¦'),
        ('business economics management finance', 'ç»æµç®¡ç†'),
        ('computer technology programming ai', 'ç§‘æŠ€'),
        ('art design music', 'è‰ºæœ¯è®¾è®¡'),
        ('health cooking lifestyle', 'ç”Ÿæ´»'),
    ]
    for keys, category in mapping:
        for keyword in keys.split():
            if keyword in joined:
                return category
    return 'æ–‡å­¦å°è¯´'


def autocomplete_book(query):
    if not query:
        return []
    suggestions = []
    seen = set()

    try:
        fields = 'title,author_name,first_publish_year'
        ol_url = f"https://openlibrary.org/search.json?q={urllib.parse.quote(query)}&limit=8&fields={fields}"
        ol_data = fetch_json(ol_url, timeout=5)
        for doc in ol_data.get('docs', [])[:8]:
            title = str(doc.get('title', '')).strip()
            if not title:
                continue
            author = ', '.join(doc.get('author_name', [])[:2]) if doc.get('author_name') else ''
            key = normalize_key(title, author)
            if key in seen:
                continue
            seen.add(key)
            suggestions.append({
                'title': title,
                'author': author,
                'year': doc.get('first_publish_year'),
                'source': 'Open Library'
            })
    except Exception:
        pass

    try:
        gb_query = urllib.parse.quote(query)
        gb_url = f"https://www.googleapis.com/books/v1/volumes?q={gb_query}&maxResults=8&printType=books"
        gb_data = fetch_json(gb_url, timeout=5)
        for item in (gb_data.get('items', []) or [])[:8]:
            volume = item.get('volumeInfo', {})
            title = str(volume.get('title', '')).strip()
            if not title:
                continue
            author = ', '.join(volume.get('authors', [])[:2]) if volume.get('authors') else ''
            year = None
            published_date = str(volume.get('publishedDate', ''))
            if published_date[:4].isdigit():
                year = int(published_date[:4])
            key = normalize_key(title, author)
            if key in seen:
                continue
            seen.add(key)
            suggestions.append({
                'title': title,
                'author': author,
                'year': year,
                'source': 'Google Books'
            })
    except Exception:
        pass

    return suggestions[:10]


def read_data():
    """è¯»å–æ•°æ®æ–‡ä»¶"""
    os.makedirs(os.path.dirname(DATA_FILE), exist_ok=True)
    if not os.path.exists(DATA_FILE):
        initial = {"books": []}
        with open(DATA_FILE, 'w', encoding='utf-8') as f:
            json.dump(initial, f, ensure_ascii=False, indent=2)
        return initial
    with open(DATA_FILE, 'r', encoding='utf-8') as f:
        return json.load(f)


def write_data(data):
    """å†™å…¥æ•°æ®æ–‡ä»¶"""
    os.makedirs(os.path.dirname(DATA_FILE), exist_ok=True)
    with open(DATA_FILE, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)


class BookHandler(http.server.SimpleHTTPRequestHandler):
    """å¤„ç† API å’Œé™æ€æ–‡ä»¶è¯·æ±‚"""

    def __init__(self, *args, **kwargs):
        super().__init__(*args, directory=PUBLIC_DIR, **kwargs)

    def log_message(self, format, *args):
        """ç®€åŒ–æ—¥å¿—è¾“å‡º"""
        pass

    def send_json(self, data, status=200):
        """å‘é€ JSON å“åº”"""
        body = json.dumps(data, ensure_ascii=False, indent=2).encode('utf-8')
        self.send_response(status)
        self.send_header('Content-Type', 'application/json; charset=utf-8')
        self.send_header('Content-Length', len(body))
        self.end_headers()
        self.wfile.write(body)

    def read_body(self):
        """è¯»å–è¯·æ±‚ä½“ JSON"""
        length = int(self.headers.get('Content-Length', 0))
        if length == 0:
            return {}
        body = self.rfile.read(length)
        return json.loads(body.decode('utf-8'))

    def do_GET(self):
        parsed = urlparse(self.path)
        path = parsed.path
        query_params = parse_qs(parsed.query)

        if path == '/api/books':
            data = read_data()
            self.send_json(data['books'])
        elif path == '/api/search-book':
            # æœç´¢ä¹¦ç±ä¿¡æ¯
            title = query_params.get('title', [''])[0]
            author = query_params.get('author', [''])[0]
            if not title:
                self.send_json({"error": "è¯·æä¾›ä¹¦å"}, 400)
                return
            results = search_book_info(title, author)
            self.send_json(results)
        elif path == '/api/search-suggest':
            query = query_params.get('q', [''])[0].strip()
            if len(query) < 2:
                self.send_json([])
                return
            self.send_json(autocomplete_book(query))
        elif path.startswith('/api/'):
            self.send_json({"error": "æœªæ‰¾åˆ°"}, 404)
        else:
            # é™æ€æ–‡ä»¶
            super().do_GET()

    def do_POST(self):
        parsed = urlparse(self.path)
        path = parsed.path

        # æ·»åŠ ä¹¦ç±
        if path == '/api/books':
            body = self.read_body()
            data = read_data()
            book = {
                "id": str(uuid.uuid4()),
                "title": body.get("title", ""),
                "author": body.get("author", ""),
                "synopsis": body.get("synopsis", ""),
                "rating": body.get("rating"),
                "ratingSource": body.get("ratingSource", ""),
                "category": body.get("category", "æœªåˆ†ç±»"),
                "cover": body.get("cover", ""),
                "resources": body.get("resources", []),
                "addedBy": body.get("addedBy", "åŒ¿å"),
                "addedAt": datetime.now(timezone.utc).isoformat(),
                "status": "candidate",
                "votes": {},
                "reviews": []
            }
            data['books'].append(book)
            write_data(data)
            self.send_json(book)
            return

        # æŠ•ç¥¨
        parts = path.strip('/').split('/')
        if len(parts) == 4 and parts[0] == 'api' and parts[1] == 'books' and parts[3] == 'vote':
            book_id = parts[2]
            body = self.read_body()
            data = read_data()
            book = next((b for b in data['books'] if b['id'] == book_id), None)
            if not book:
                self.send_json({"error": "ä¹¦ç±æœªæ‰¾åˆ°"}, 404)
                return
            user_id = body.get("userId", "åŒ¿å")
            if user_id in book.get('votes', {}):
                del book['votes'][user_id]
            else:
                if 'votes' not in book:
                    book['votes'] = {}
                book['votes'][user_id] = True
            write_data(data)
            self.send_json(book)
            return

        # æ·»åŠ ä¹¦è¯„
        if len(parts) == 4 and parts[0] == 'api' and parts[1] == 'books' and parts[3] == 'reviews':
            book_id = parts[2]
            body = self.read_body()
            data = read_data()
            book = next((b for b in data['books'] if b['id'] == book_id), None)
            if not book:
                self.send_json({"error": "ä¹¦ç±æœªæ‰¾åˆ°"}, 404)
                return
            review = {
                "id": str(uuid.uuid4()),
                "userId": body.get("userId", "åŒ¿å"),
                "content": body.get("content", ""),
                "rating": body.get("rating"),
                "createdAt": datetime.now(timezone.utc).isoformat(),
                "comments": []
            }
            if 'reviews' not in book:
                book['reviews'] = []
            book['reviews'].append(review)
            write_data(data)
            self.send_json(review)
            return

        # æ·»åŠ è¯„è®ºåˆ°ä¹¦è¯„
        if len(parts) == 6 and parts[0] == 'api' and parts[1] == 'books' and parts[3] == 'reviews' and parts[5] == 'comments':
            book_id = parts[2]
            review_id = parts[4]
            body = self.read_body()
            data = read_data()
            book = next((b for b in data['books'] if b['id'] == book_id), None)
            if not book:
                self.send_json({"error": "ä¹¦ç±æœªæ‰¾åˆ°"}, 404)
                return
            review = next((r for r in book.get('reviews', []) if r['id'] == review_id), None)
            if not review:
                self.send_json({"error": "ä¹¦è¯„æœªæ‰¾åˆ°"}, 404)
                return
            comment = {
                "id": str(uuid.uuid4()),
                "userId": body.get("userId", "åŒ¿å"),
                "content": body.get("content", ""),
                "createdAt": datetime.now(timezone.utc).isoformat()
            }
            if 'comments' not in review:
                review['comments'] = []
            review['comments'].append(comment)
            write_data(data)
            self.send_json(comment)
            return

        self.send_json({"error": "æœªæ‰¾åˆ°"}, 404)

    def do_PUT(self):
        parsed = urlparse(self.path)
        path = parsed.path
        parts = path.strip('/').split('/')

        # æ›´æ–°ä¹¦ç±
        if len(parts) == 3 and parts[0] == 'api' and parts[1] == 'books':
            book_id = parts[2]
            body = self.read_body()
            data = read_data()
            book = next((b for b in data['books'] if b['id'] == book_id), None)
            if not book:
                self.send_json({"error": "ä¹¦ç±æœªæ‰¾åˆ°"}, 404)
                return
            allowed = ['title', 'author', 'synopsis', 'rating', 'ratingSource', 'category', 'cover', 'status']
            for key in allowed:
                if key in body:
                    book[key] = body[key]
            write_data(data)
            self.send_json(book)
            return

        self.send_json({"error": "æœªæ‰¾åˆ°"}, 404)

    def do_DELETE(self):
        parsed = urlparse(self.path)
        path = parsed.path
        parts = path.strip('/').split('/')

        # åˆ é™¤ä¹¦ç±
        if len(parts) == 3 and parts[0] == 'api' and parts[1] == 'books':
            book_id = parts[2]
            data = read_data()
            idx = next((i for i, b in enumerate(data['books']) if b['id'] == book_id), None)
            if idx is None:
                self.send_json({"error": "ä¹¦ç±æœªæ‰¾åˆ°"}, 404)
                return
            removed = data['books'].pop(idx)
            write_data(data)
            self.send_json(removed)
            return

        # åˆ é™¤ä¹¦è¯„
        if len(parts) == 5 and parts[0] == 'api' and parts[1] == 'books' and parts[3] == 'reviews':
            book_id = parts[2]
            review_id = parts[4]
            data = read_data()
            book = next((b for b in data['books'] if b['id'] == book_id), None)
            if not book:
                self.send_json({"error": "ä¹¦ç±æœªæ‰¾åˆ°"}, 404)
                return
            idx = next((i for i, r in enumerate(book.get('reviews', [])) if r['id'] == review_id), None)
            if idx is None:
                self.send_json({"error": "ä¹¦è¯„æœªæ‰¾åˆ°"}, 404)
                return
            book['reviews'].pop(idx)
            write_data(data)
            self.send_json({"success": True})
            return

        # åˆ é™¤è¯„è®º
        if len(parts) == 7 and parts[0] == 'api' and parts[1] == 'books' and parts[3] == 'reviews' and parts[5] == 'comments':
            book_id = parts[2]
            review_id = parts[4]
            comment_id = parts[6]
            data = read_data()
            book = next((b for b in data['books'] if b['id'] == book_id), None)
            if not book:
                self.send_json({"error": "ä¹¦ç±æœªæ‰¾åˆ°"}, 404)
                return
            review = next((r for r in book.get('reviews', []) if r['id'] == review_id), None)
            if not review:
                self.send_json({"error": "ä¹¦è¯„æœªæ‰¾åˆ°"}, 404)
                return
            idx = next((i for i, c in enumerate(review.get('comments', [])) if c['id'] == comment_id), None)
            if idx is None:
                self.send_json({"error": "è¯„è®ºæœªæ‰¾åˆ°"}, 404)
                return
            review['comments'].pop(idx)
            write_data(data)
            self.send_json({"success": True})
            return

        self.send_json({"error": "æœªæ‰¾åˆ°"}, 404)


class ThreadedServer(socketserver.ThreadingMixIn, http.server.HTTPServer):
    """æ”¯æŒå¤šçº¿ç¨‹çš„ HTTP æœåŠ¡å™¨"""
    allow_reuse_address = True


if __name__ == '__main__':
    server = ThreadedServer(('0.0.0.0', PORT), BookHandler)
    print(f'ğŸ“š é˜…è¯»è®¡åˆ’ç®¡ç†å·¥å…·å·²å¯åŠ¨!')
    print(f'   æœ¬åœ°è®¿é—®: http://localhost:{PORT}')
    print(f'   æŒ‰ Ctrl+C åœæ­¢æœåŠ¡å™¨')
    try:
        server.serve_forever()
    except KeyboardInterrupt:
        print('\nğŸ‘‹ æœåŠ¡å™¨å·²åœæ­¢')
        server.server_close()
